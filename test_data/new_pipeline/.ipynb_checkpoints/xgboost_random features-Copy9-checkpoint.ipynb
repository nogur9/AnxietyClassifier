{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.decomposition import PCA\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import StratifiedKFold, GridSearchCV, train_test_split, cross_val_score\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.model_selection import LeaveOneOut\n",
    "from imblearn.over_sampling import SMOTE, ADASYN\n",
    "#from imblearn.pipeline import Pipeline\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RemoveCorrelationTransformer2(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, correlation_threshold=0.7):\n",
    "        self.correlation_threshold = correlation_threshold\n",
    "\n",
    "\n",
    "    def fit(self, X, Y=None):\n",
    "        df = pd.DataFrame(X)\n",
    "        df_corr = df.corr(method='pearson', min_periods=1)\n",
    "        df_not_correlated = ~(df_corr.mask(\n",
    "            np.tril(np.ones([len(df_corr)] * 2, dtype=bool))).abs() > self.correlation_threshold).any()\n",
    "        self.un_corr_idx = df_not_correlated.loc[df_not_correlated[df_not_correlated.index] == True].index\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, Y=None):\n",
    "        df = pd.DataFrame(X)\n",
    "        df = df[self.un_corr_idx]\n",
    "        return df.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class RemoveCorrelationTransformer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, correlation_threshold=0.7, pca_components_ratio=3):\n",
    "        self.correlation_threshold = correlation_threshold\n",
    "        self.pca_components_ratio = pca_components_ratio\n",
    "\n",
    "\n",
    "    def fit(self, X, Y=None):\n",
    "        df = pd.DataFrame(X)\n",
    "        df_corr = df.corr(method='pearson')\n",
    "        df_corr = df_corr - np.eye(df.shape[1])\n",
    "        outliares_corr = df_corr[np.abs(df_corr) > self.correlation_threshold]\n",
    "        self.outliares_corr = outliares_corr.dropna(axis=1, how='all')\n",
    "\n",
    "        correlated_df = df[self.outliares_corr.columns]\n",
    "\n",
    "        n_components = len(self.outliares_corr.columns) // self.pca_components_ratio\n",
    "        pca = PCA(n_components=n_components)\n",
    "\n",
    "        correlated_df = pca.fit_transform(correlated_df)\n",
    "        self.correlated_df = pd.DataFrame(correlated_df, columns=[\"pca_{}\".format(i) for i in range(n_components)])\n",
    "\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, Y=None):\n",
    "        df = pd.DataFrame(X)\n",
    "        print(self.outliares_corr.columns)\n",
    "        df = df.drop((self.outliares_corr.columns), axis=1)\n",
    "        df = df.join(self.correlated_df)\n",
    "        return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class RemoveMissingFeaturesTransformer(BaseEstimator, TransformerMixin):\n",
    "\n",
    "    def fit(self, X, Y=None):\n",
    "        self.is_missing = X.isnull().values.any(axis=0)\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, Y=None):\n",
    "        copy_x = pd.DataFrame(X)\n",
    "        self.is_missing += copy_x.isnull().values.any(axis=0)\n",
    "\n",
    "        copy_x = copy_x.iloc[:, ~self.is_missing]\n",
    "\n",
    "        return copy_x.values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def refactor_labels(df):\n",
    "    return df.replace({'low': 0 ,'high': 1, 'clinical': 1 })\n",
    "\n",
    "\n",
    "def get_data(file_name, LSAS_threshold=None):\n",
    "    group_column = 'group'\n",
    "    sub_num_col = 'Subject_Number'\n",
    "    lsas_col = 'LSAS'\n",
    "    \n",
    "#     corr_remove = ['STD_fixation_length_Disgusted', 'average_pupil_size_All',\n",
    "#        'STD_pupil_size_Disgusted', 'STD_pupil_size_White_Space',\n",
    "#        'average_pupil_size_Disgusted', 'average_pupil_size_Neutral',\n",
    "#        'Ratio D/D+N', 'STD_fixation_length_Neutral', 'STD_pupil_size_All',\n",
    "#        'average_pupil_size_White_Space',\n",
    "#        'avg_of_sum_fixation_length_White_Space']\n",
    "    \n",
    "    df = pd.read_excel(file_name, sheet_name='Sheet1')\n",
    "    if LSAS_threshold is None:\n",
    "        X = df.drop([group_column, sub_num_col, lsas_col], 1)\n",
    "        Y = refactor_labels(df[group_column])\n",
    "        return X, Y\n",
    "    else:\n",
    "        \n",
    "        X = df.drop([group_column], 1)\n",
    "        Y = pd.Series(np.where(X[lsas_col] > LSAS_threshold, 1, 0))\n",
    "        X = X.drop([sub_num_col, lsas_col], 1)\n",
    "        return X, Y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = \"training_set_100.xlsx\"\n",
    "X_train, y_train = get_data(file_name, LSAS_threshold = 50)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "amount_features 3\n",
      "['STD_fixation_length_Disgusted', 'STD_pupil_size_Disgusted', 'mean_different_AOI_per_trial']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda\\lib\\site-packages\\sklearn\\model_selection\\_search.py:841: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "________________\n",
      "best params\n",
      " {'classifier__n_estimators': 200}\n",
      "\n",
      "best score\n",
      " 0.5777777777777777\n"
     ]
    }
   ],
   "source": [
    "for i in range(300):\n",
    "    amount_features = 3 + (i//25)\n",
    "    print(\"\\namount_features\", amount_features)\n",
    "    #which features to take\n",
    "\n",
    "    columns_shuffled = list(X_train.columns)\n",
    "    random.shuffle(columns_shuffled)\n",
    "    print(columns_shuffled[:amount_features])\n",
    "\n",
    "    \n",
    "    X_train = X_train[columns_shuffled[:amount_features]]\n",
    "    \n",
    "    X_train_2, X_test_2, y_train_2, y_test_2 = train_test_split(X_train, y_train, test_size = 0.1, stratify=y_train)\n",
    "    \n",
    "    \n",
    "    pipe =  Pipeline([\n",
    "    ('classifier', RandomForestClassifier())\n",
    "    ])\n",
    "    \n",
    "    params_grid = [\n",
    "    {\n",
    "        #'classifier': [RandomForestClassifier(), GradientBoostingClassifier(), AdaBoostClassifier]\n",
    "        'classifier__n_estimators': [75, 100,200]\n",
    "    }] \n",
    "    \n",
    "    \n",
    "    cv = StratifiedKFold(10)\n",
    "    gs = GridSearchCV(pipe, params_grid, cv=cv, scoring='accuracy')\n",
    "    gs.fit(X_train_2, y_train_2);\n",
    "\n",
    "\n",
    "    print(\"________________\\nbest params\\n\",gs.best_params_)\n",
    "    print(\"\\nbest score\\n\",gs.best_score_)\n",
    "\n",
    "    results = []\n",
    "    k = 15\n",
    "    for i in range(k):\n",
    "        loo = LeaveOneOut()\n",
    "        score = cross_val_score(gs.best_estimator_, X_train_2, y_train_2, cv=loo)\n",
    "        results.append(score.mean())\n",
    "    print(\"avg acc in train2 set\\n\",sum(results)/k)\n",
    "\n",
    "    model = gs.best_estimator_.fit(X_train_2, y_train_2)\n",
    "    y_pred = model.predict(X_test_2)\n",
    "    print (\"acc on holdout\\n\", accuracy_score(y_pred, y_test_2))\n",
    "    \n",
    "    results = []\n",
    "    for i in range(k):\n",
    "        loo = LeaveOneOut()\n",
    "        score = cross_val_score(gs.best_estimator_, X_train, y_train, cv=loo)\n",
    "        results.append(score.mean())\n",
    "    print(\"avg acc in train1 set\\n\",sum(results)/k, \"\\n\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "? AdaBoostClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
